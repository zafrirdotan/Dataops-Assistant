import json
import os
import re
from urllib import response
from app.services.llm_service import LLMService
from app.utils.json_utils import safe_json_dumps

ALLOWED_PACKAGES = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "python-dotenv>=1.0.0",
    "pyarrow>=14.0.0",
    "pytest>=7.0.0"
]
class PipelineCodeGenerator:
    """
    Service for retrieving a data sample and generating transformation code using LLM.
    """
    def __init__(self):
        self.llm = LLMService()

    def generate_code(self, spec: dict, data_preview: dict, last_code: str = None, last_error: str = None, python_test: str = None) -> str:
        """
        Generate transformation code including data loading, transformation, and saving
        as well as unit tests.
        Args:
            spec (dict): The pipeline specification.
            db_info (dict): Information about the source/destination databases.
        Returns:
            str: Generated transformation code.
        """

        pipeline_name = spec.get("pipeline_name")

        print(f"data_preview: {data_preview}")  # For debugging purposes
        # Construct the prompt for the LLM
        # Todo: improve the file path handling
        prompt = f"""
            Use Python 3.13 and best practices to generate the code.
            The allowed packages are: {', '.join(ALLOWED_PACKAGES)}.

            Given the following pipeline specification:
            {safe_json_dumps(spec, indent=2)}

            And the following data preview:
            {safe_json_dumps(data_preview, indent=2)}

            All generated files—the main code (`{pipeline_name}.py`), the requirements file (`requirements.txt`), and the unit test (`{pipeline_name}_test.py`)—should be placed in the same folder: `../pipelines/{pipeline_name}/`.
            In the unit test, import functions from `{pipeline_name}` (e.g., `from {pipeline_name} import ...`).

            The ETL pipeline must ingest all available data from the source files, regardless of the number of records or partitions.
            If partitioning Parquet files, use a strategy that avoids exceeding system limits (e.g., group by year or month if there are too many unique dates, or write without partitioning if necessary).

            The input data files path should be loaded from a .env file using the variable DATA_ROUTE. In the generated code, use:

            from dotenv import load_dotenv
            import os
            load_dotenv()
            DATA_FOLDER = os.getenv('DATA_FOLDER')

            Use DATA_FOLDER as the path for input data files in all relevant code.

            Ensure the DataFrame has a 'date' column. If not, add today's date.

            All output files generated by the pipeline must be saved to the output folder inside the pipeline directory: '../pipelines/{pipeline_name}/output/'.
            Make sure this folder exists before writing output files.

            Generate Python code to perform the specified transformations and load the data into the destination.
            Also, generate a requirements.txt listing all necessary Python packages.
            Additionally, generate a small unit test (using pytest) that verifies the output of the main transformation function to ensure it works correctly. The unit test should be returned as a third code block (```python test ... ```).
            Return only three code blocks: one with Python code (```python ... ```), one with requirements.txt (```requirements.txt ... ```), and one with the unit test (```python test ... ```).
            Do not include explanations or extra text.
            """
        
        if last_code and last_error:
            prompt += f"""
            The last generated code had the following error when executed:
            {last_error}

            Here is the last generated code:
            {last_code}
            This is the test code:
            {python_test}

            Please fix the code to resolve the error.
            """

# ...existing code...

        prompt_v2 = f"""
                ## CONTEXT
                You are an expert Python 3.13 engineer generating production-quality ETL pipeline code.

                **Pipeline Specification:**
                {safe_json_dumps(spec, indent=2)}

                **Data Preview:**
                {safe_json_dumps(data_preview, indent=2)}

                ## CONSTRAINTS
                - **Python Version:** 3.13 with best practices
                - **Allowed Packages:** {', '.join(ALLOWED_PACKAGES)}
                - **Code Quality:** Use type hints, modular structure, and error handling

                ## FILE STRUCTURE REQUIREMENTS
                - **Main Code:** `../pipelines/{pipeline_name}/{pipeline_name}.py`
                - **Requirements:** `../pipelines/{pipeline_name}/requirements.txt`
                - **Unit Test:** `../pipelines/{pipeline_name}/{pipeline_name}_test.py`
                - **Output Directory:** `../pipelines/{pipeline_name}/output/`
                - Ensure this folder exists before writing files

                ## DATA HANDLING REQUIREMENTS
                ### Input Data Loading
                ```python
                from dotenv import load_dotenv
                import os
                load_dotenv()
                DATA_FOLDER = os.getenv('DATA_FOLDER')
                ```
                - Use `DATA_FOLDER` for all input file paths

                ### ETL Processing
                - **Data Ingestion:** Process ALL available data regardless of records/partitions
                - **Date Column:** Ensure DataFrame has 'date' column (add today's date if missing)
                - **Parquet Partitioning:** 
                - Prefer year/month grouping to avoid system limits
                - Write without partitioning if too many unique dates

                ## TESTING REQUIREMENTS
                - **Framework:** pytest
                - **Import Pattern:** `from {pipeline_name} import ...`
                - **Coverage:** Test main transformation function for correctness

                ## OUTPUT FORMAT
                Return exactly three code blocks in this order:
                1. ```python
                [main pipeline code]
                ```
                2. ```requirements.txt
                [package dependencies]
                ```
                3. ```python test
                [unit test code]
                ```

                **Important:** Return ONLY the three code blocks. No explanations or extra text.
                """

        # Error handling section
        if last_code and last_error:
            prompt_v2 += f"""
                ## ERROR CORRECTION
                The previous code execution failed. Please fix the following:

                **Error:**
                {last_error}

                **Previous Code:**
                {last_code}

                **Previous Test:**
                {python_test}

                Fix the issues and regenerate all three code blocks.
                """

# ...existing code...
        # prompt = f"""
        #     You are an expert Python 3.13 engineer. 
        #     Generate production-quality code following best practices. 

        #     Constraints:
        #     - Allowed packages: {', '.join(ALLOWED_PACKAGES)}
        #     - Use idiomatic Python, type hints, and clear modular structure.
        #     - Ensure reproducibility and robustness in file handling.

        #     Context:
        #     Pipeline specification:
        #     {safe_json_dumps(spec, indent=2)}

        #     Data preview:
        #     {safe_json_dumps(data_preview, indent=2)}

        #     Requirements:
        #     1. **File structure**:
        #     - Place all generated files in: `../pipelines/{pipeline_name}/`
        #     - Output files must be written to: `../pipelines/{pipeline_name}/output/`
        #     - Ensure the output folder exists before writing.

        #     2. **Environment variables**:
        #     - Input data path must be loaded from `.env`:
        #         ```python
        #         from dotenv import load_dotenv
        #         import os
        #         load_dotenv()
        #         DATA_FOLDER = os.getenv("DATA_FOLDER")
        #         ```
        #     - Always use `DATA_FOLDER` for reading input files.

        #     3. **ETL pipeline**:
        #     - Ingest all available data, regardless of records or partitions.
        #     - For Parquet partitioning, avoid system limits:
        #         * Prefer partitioning by year/month.
        #         * If too many partitions, write without partitioning.
        #     - Ensure the DataFrame has a `date` column. If missing, add today’s date.

        #     4. **Testing**:
        #     - Provide a unit test using `pytest`.
        #     - Test must import from `{pipeline_name}` (e.g., `from {pipeline_name} import ...`).
        #     - Test should verify correctness of the main transformation function.

        #     5. **Outputs**:
        #     - Return exactly three code blocks, in this order:
        #         1. Main Python code (` ```python ... ``` `)
        #         2. `requirements.txt` (` ```requirements.txt ... ``` `)
        #         3. Unit test (` ```python test ... ``` `)

        #     6. **Formatting**:
        #     - Do not include explanations, comments outside code, or extra text.

        #     Error handling:
        #     If provided, here is the last execution error and code. Fix the problem in the new version:
        #     Error:
        #     {last_error if last_error else "None"}

        #     Last generated code:
        #     {last_code if last_code else "None"}

        #     Last unit test:
        #     {python_test if python_test else "None"}
        #     """
        response = self.llm.response_create(
            model="gpt-4.1",
            input=prompt_v2,
            temperature=0,
        )

        python_code = self.extract_code_block(response.output_text, "python")
        requirements = self.extract_code_block(response.output_text, "requirements.txt")
        python_test = self.extract_code_block(response.output_text, "python test")

        if not self.check_requirements(requirements):
            raise ValueError("Generated requirements.txt contains disallowed packages.")

        return python_code, requirements, python_test

    def extract_code_block(self, llm_response: str, block_type: str) -> str:
        # Extract code between triple backticks with block_type
        pattern = rf"```{block_type}(.*?)```"
        match = re.search(pattern, llm_response, re.DOTALL)
        if match:
            return match.group(1).strip()
        return ""
    
    def check_requirements(self, requirements: str) -> bool:
        # Basic check to ensure requirements.txt is not empty and has valid format
        # Then check against a whitelist of allowed packages (for security)
        if not requirements.strip():
            return False
        lines = requirements.strip().split("\n")
        for line in lines:
            pkg = line.split("==")[0].strip().lower()
            if pkg not in ALLOWED_PACKAGES:
                return False
        return True
    