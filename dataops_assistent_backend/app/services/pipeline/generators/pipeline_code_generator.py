import os
import re
from app.services.llm_service import LLMService
from app.utils.json_utils import safe_json_dumps

ALLOWED_PACKAGES = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "python-dotenv>=1.0.0",
    "pyarrow>=14.0.0",
    "pytest>=7.0.0",
    "sqlalchemy>=2.0.0",  # Add this for PostgreSQL support
    "psycopg2-binary>=2.9.0"  # Add this for PostgreSQL driver
]
class PipelineCodeGenerator:
    """
    Service for retrieving a data sample and generating transformation code using LLM.
    """
    def __init__(self):
        self.llm = LLMService()

    def generate_code(self, spec: dict, data_preview: dict, last_code: str = None, last_error: str = None, python_test: str = None) -> str:
        """
        Generate transformation code including data loading, transformation, and saving
        as well as unit tests.
        Args:
            spec (dict): The pipeline specification.
            db_info (dict): Information about the source/destination databases.
        Returns:
            str: Generated transformation code.
        """

        pipeline_name = spec.get("pipeline_name")

        print(f"data_preview: {data_preview}")  # For debugging purposes
        # Construct the prompt for the LLM
        # Todo: improve the file path handling
        prompt = f"""
            Use Python 3.13 and best practices to generate the code.
            The allowed packages are: {', '.join(ALLOWED_PACKAGES)}.

            Given the following pipeline specification:
            {safe_json_dumps(spec, indent=2)}

            And the following data preview:
            {safe_json_dumps(data_preview, indent=2)}

            All generated files—the main code (`{pipeline_name}.py`), the requirements file (`requirements.txt`), and the unit test (`{pipeline_name}_test.py`)—should be placed in the same folder: `../pipelines/{pipeline_name}/`.
            In the unit test, import functions from `{pipeline_name}` (e.g., `from {pipeline_name} import ...`).

            The ETL pipeline must ingest all available data from the source files, regardless of the number of records or partitions.
            If partitioning Parquet files, use a strategy that avoids exceeding system limits (e.g., group by year or month if there are too many unique dates, or write without partitioning if necessary).

            The input data files path should be loaded from a .env file using the variable DATA_ROUTE. In the generated code, use:

            from dotenv import load_dotenv
            import os
            load_dotenv()
            DATA_FOLDER = os.getenv('DATA_FOLDER')
            OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')

            Use DATA_FOLDER as the path for input data files in all relevant code.

            Ensure the DataFrame has a 'date' column. If not, add today's date.

            All output files generated by the pipeline must be saved to the output folder inside the pipeline directory: '../pipelines/{pipeline_name}/output/'.
            Make sure this folder exists before writing output files.

            Generate Python code to perform the specified transformations and load the data into the destination.
            Also, generate a requirements.txt listing all necessary Python packages.
            Additionally, generate a small unit test (using pytest) that verifies the output of the main transformation function to ensure it works correctly. The unit test should be returned as a third code block (```python test ... ```).
            Return only three code blocks: one with Python code (```python ... ```), one with requirements.txt (```requirements.txt ... ```), and one with the unit test (```python test ... ```).
            Do not include explanations or extra text.
            """
        
        if last_code and last_error:
            prompt += f"""
            The last generated code had the following error when executed:
            {last_error}

            Here is the last generated code:
            {last_code}
            This is the test code:
            {python_test}

            Please fix the code to resolve the error.
            """

# ...existing code...
        prompt_v2 = f"""
                ## CONTEXT
                You are an expert Python 3.13 engineer generating production-quality ETL pipeline code.

                **Pipeline Specification:**
                {safe_json_dumps(spec, indent=2)}

                **Data Preview:**
                {safe_json_dumps(data_preview, indent=2)}

                ## CONSTRAINTS
                - **Python Version:** 3.13 with best practices
                - **Allowed Packages:** {', '.join(ALLOWED_PACKAGES)}
                - **Code Quality:** Use type hints, modular structure, and error handling

                ## FILE STRUCTURE REQUIREMENTS
                - **Main Code:** `../pipelines/{pipeline_name}/{pipeline_name}.py`
                - **Requirements:** `../pipelines/{pipeline_name}/requirements.txt`
                - **Unit Test:** `../pipelines/{pipeline_name}/{pipeline_name}_test.py`
                - Ensure this folder exists before writing files
                - **Output Folder:** All output files must be saved to `../pipelines/{pipeline_name}/output/`
                
                ## DATA HANDLING REQUIREMENTS
                ### Input Data Loading
                ```python
                from dotenv import load_dotenv
                import os
                load_dotenv()
                DATA_FOLDER = os.getenv('DATA_FOLDER')
                OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')
                ```
                - Use `DATA_FOLDER` for all input file paths

                ### ETL Processing
                - **Data Ingestion:** Process ALL available data regardless of records/partitions
                - **Date Column:** Ensure DataFrame has 'date' column (add today's date if missing)
                - **Parquet Partitioning:** 
                - Prefer year/month grouping to avoid system limits
                - Write without partitioning if too many unique dates

                ## TESTING REQUIREMENTS
                - **Framework:** pytest
                - **Import Pattern:** `from {pipeline_name} import ...`
                - **Coverage:** Test main transformation function for correctness

                ## OUTPUT FORMAT
                Return exactly three code blocks in this order:
                1. ```python
                [main pipeline code]
                ```
                2. ```requirements.txt
                [package dependencies]
                ```
                3. ```python test
                [unit test code]
                ```

                **Important:** Return ONLY the three code blocks. No explanations or extra text.
                """

        # Error handling section
        if last_code and last_error:
            prompt_v2 += f"""
                ## ERROR CORRECTION
                The previous code execution failed. Please fix the following:

                **Error:**
                {last_error}

                **Previous Code:**
                {last_code}

                **Previous Test:**
                {python_test}

                Fix the issues and regenerate all three code blocks.
                """

        # V3 Prompt with conditional configuration
        prompt_v3 = self._create_conditional_prompt(spec, data_preview, pipeline_name, last_code, last_error, python_test)

# ...existing code...
        # prompt = f"""
        #     You are an expert Python 3.13 engineer. 
        #     Generate production-quality code following best practices. 

        #     Constraints:
        #     - Allowed packages: {', '.join(ALLOWED_PACKAGES)}
        #     - Use idiomatic Python, type hints, and clear modular structure.
        #     - Ensure reproducibility and robustness in file handling.

        #     Context:
        #     Pipeline specification:
        #     {safe_json_dumps(spec, indent=2)}

        #     Data preview:
        #     {safe_json_dumps(data_preview, indent=2)}

        #     Requirements:
        #     1. **File structure**:
        #     - Place all generated files in: `../pipelines/{pipeline_name}/`
        #     - Output files must be written to: `../pipelines/{pipeline_name}/output/`
        #     - Ensure the output folder exists before writing.

        #     2. **Environment variables**:
        #     - Input data path must be loaded from `.env`:
        #         ```python
        #         from dotenv import load_dotenv
        #         import os
        #         load_dotenv()
        #         DATA_FOLDER = os.getenv("DATA_FOLDER")
        #         ```
        #     - Always use `DATA_FOLDER` for reading input files.

        #     3. **ETL pipeline**:
        #     - Ingest all available data, regardless of records or partitions.
        #     - For Parquet partitioning, avoid system limits:
        #         * Prefer partitioning by year/month.
        #         * If too many partitions, write without partitioning.
        #     - Ensure the DataFrame has a `date` column. If missing, add today’s date.

        #     4. **Testing**:
        #     - Provide a unit test using `pytest`.
        #     - Test must import from `{pipeline_name}` (e.g., `from {pipeline_name} import ...`).
        #     - Test should verify correctness of the main transformation function.

        #     5. **Outputs**:
        #     - Return exactly three code blocks, in this order:
        #         1. Main Python code (` ```python ... ``` `)
        #         2. `requirements.txt` (` ```requirements.txt ... ``` `)
        #         3. Unit test (` ```python test ... ``` `)

        #     6. **Formatting**:
        #     - Do not include explanations, comments outside code, or extra text.

        #     Error handling:
        #     If provided, here is the last execution error and code. Fix the problem in the new version:
        #     Error:
        #     {last_error if last_error else "None"}

        #     Last generated code:
        #     {last_code if last_code else "None"}

        #     Last unit test:
        #     {python_test if python_test else "None"}
        #     """
        response = self.llm.response_create(
            model="gpt-4.1",
            input=prompt_v3,  # Using v3 prompt with conditional configuration
            temperature=0,
        )

        python_code = self.extract_code_block(response.output_text, "python")
        requirements = self.extract_code_block(response.output_text, "requirements.txt")
        python_test = self.extract_code_block(response.output_text, "python test")

        if self.check_requirements(requirements) != True:
            disallowed = self.check_requirements(requirements)

            raise ValueError(f"Generated requirements.txt contains disallowed packages: {disallowed}")

        return python_code, requirements, python_test

    def extract_code_block(self, llm_response: str, block_type: str) -> str:
        # Extract code between triple backticks with block_type
        pattern = rf"```{block_type}(.*?)```"
        match = re.search(pattern, llm_response, re.DOTALL)
        if match:
            return match.group(1).strip()
        return ""
    
    def check_requirements(self, requirements: str) -> bool | list:
        # Return True if all packages are allowed, otherwise False
        if not requirements.strip():
            return False
        lines = requirements.strip().split("\n")
        disallowed = []
        allowed_pkgs = [pkg.split(">=")[0].split("==")[0].strip().lower() for pkg in ALLOWED_PACKAGES]
        for line in lines:
            pkg_name = line.split(">=")[0].split("==")[0].strip().lower()
            if pkg_name and pkg_name not in allowed_pkgs:
                disallowed.append(line.strip())
        if disallowed:
            print(f"Disallowed packages found: {disallowed}")
            return disallowed
        return True

    def _create_conditional_prompt(self, spec: dict, data_preview: dict, pipeline_name: str, 
                                   last_code: str = None, last_error: str = None, python_test: str = None) -> str:
        """
        Create v3 prompt with conditional configuration based on source_type and destination_type
        """
        input_config = self._generate_input_config(spec)
        output_config = self._generate_output_config(spec)
        
        prompt_v3 = f"""
        ## CONTEXT
        You are an expert Python 3.13 engineer generating production-quality ETL pipeline code.

        **Pipeline Specification:**
        {safe_json_dumps(spec, indent=2)}

        **Data Preview:**
        {safe_json_dumps(data_preview, indent=2)}

        ## CONSTRAINTS
        - **Python Version:** 3.13 with best practices
        - **Allowed Packages:** {', '.join(ALLOWED_PACKAGES)}
        - **Code Quality:** Use type hints, modular structure, and error handling

        ## FILE STRUCTURE REQUIREMENTS
        - **Main Code:** `../pipelines/{pipeline_name}/{pipeline_name}.py`
        - **Requirements:** `../pipelines/{pipeline_name}/requirements.txt`
        - **Unit Test:** `../pipelines/{pipeline_name}/{pipeline_name}_test.py`
        - Ensure this folder exists before writing files
        - **Output Folder:** All output files must be saved to `../pipelines/{pipeline_name}/output/`
        
        ## DATA SOURCE CONFIGURATION
        {input_config}
        
        ## DATA OUTPUT CONFIGURATION
        {output_config}

        ## DATA HANDLING REQUIREMENTS
        ### ETL Processing
        - **Data Ingestion:** Process ALL available data regardless of records/partitions
        - **Date Column:** Ensure DataFrame has 'date' column (add today's date if missing)
        - **Parquet Partitioning:** 
          - Prefer year/month grouping to avoid system limits
          - Write without partitioning if too many unique dates

        ## TESTING REQUIREMENTS
        - **Framework:** pytest
        - **Import Pattern:** `from {pipeline_name} import ...`
        - **Coverage:** Test main transformation function for correctness

        ## OUTPUT FORMAT
        Return exactly three code blocks in this order:
        1. ```python
        [main pipeline code]
        ```
        2. ```requirements.txt
        [package dependencies]
        ```
        3. ```python test
        [unit test code]
        ```

        **Important:** Return ONLY the three code blocks. No explanations or extra text.
        """

        # Error handling section
        if last_code and last_error:
            prompt_v3 += f"""
            ## ERROR CORRECTION
            The previous code execution failed. Please fix the following:

            **Error:**
            {last_error}

            **Previous Code:**
            {last_code}

            **Previous Test:**
            {python_test}

            Fix the issues and regenerate all three code blocks.
            """

        return prompt_v3

    def _generate_input_config(self, spec: dict) -> str:
        """Generate input configuration based on source_type"""
        source_type = spec.get('source_type', '').lower()
        
        if source_type in ['localfilecsv', 'localfilejson']:
            return f"""
        ### Input Data Loading (File-based)
        ```python
        from dotenv import load_dotenv
        import os
        load_dotenv()
        DATA_FOLDER = os.getenv('DATA_FOLDER')
        ```
        - **Source Type:** {source_type}
        - **File Path:** Use `DATA_FOLDER` for all input file paths
        - **Data Validation:** Ensure file exists and format matches source type
        - **Error Handling:** Handle file not found, encoding issues, and malformed data
        - **Processing:** Ingest ALL available data regardless of file size
        - **File Pattern:** `os.path.join(DATA_FOLDER, '{spec.get('source_path', 'input_file')}')`
            """
        
        elif source_type == 'postgresql':
            return f"""
        ### Input Data Loading (PostgreSQL Database)
        ```python
        from dotenv import load_dotenv
        import os
        load_dotenv()
        DATABASE_URL = os.getenv('DATABASE_URL')
        DATABASE_HOST = os.getenv('DATABASE_HOST')
        DATABASE_PORT = os.getenv('DATABASE_PORT')
        DATABASE_NAME = os.getenv('DATABASE_NAME')
        DATABASE_USER = os.getenv('DATABASE_USER')
        DATABASE_PASSWORD = os.getenv('DATABASE_PASSWORD')
        ```
        - **Source Type:** PostgreSQL Database
        - **Connection:** Use DATABASE_URL: `{os.getenv('DATABASE_URL', 'postgresql://dataops_user:dataops_password@localhost:5432/dataops_db')}`
        - **Alternative Connection:** Individual parameters (HOST: {os.getenv('DATABASE_HOST', 'localhost')}, PORT: {os.getenv('DATABASE_PORT', '5432')})
        - **Authentication:** DATABASE_USER and DATABASE_PASSWORD from environment
        - **Query:** Extract data from table: `{spec.get('source_table', 'source_table')}`
        - **Connection Pattern:** Use SQLAlchemy or psycopg2 with proper connection pooling
        - **Error Handling:** Handle connection failures, timeouts, and SQL errors
        - **Processing:** Use chunked reading for large datasets with `chunksize` parameter
            """
        
        else:
            return f"""
        ### Input Data Loading (Generic)
        - **Source Type:** {source_type or 'Not specified'}
        - **Default:** Use file-based loading with DATA_FOLDER
        - **Note:** Consider specifying source_type as 'localFileCSV', 'localFileJSON', or 'PostgreSQL'
            """

    def _generate_output_config(self, spec: dict) -> str:
        """Generate output configuration based on destination_type"""
        dest_type = spec.get('destination_type', '').lower()
        
        if dest_type in ['localfilecsv', 'localfilejson', 'parquet']:
            return f"""
        ### Output Data Saving (File-based)
        ```python
        OUTPUT_FOLDER = os.getenv('OUTPUT_FOLDER')
        output_path = f'../pipelines/{spec.get('pipeline_name')}/output/'
        ```
        - **Destination Type:** {dest_type}
        - **Output Path:** Save to `../pipelines/{spec.get('pipeline_name')}/output/`
        - **Environment:** Use OUTPUT_FOLDER from .env if available
        - **File Management:** Ensure output directory exists before writing
        - **Naming:** Use descriptive filenames with timestamps if needed
        - **Partitioning:** For Parquet, use year/month grouping to avoid system limits
        - **File Pattern:** `os.path.join(output_path, '{spec.get('destination_name', 'output_file')}')`
            """
        
        elif dest_type == 'postgresql':
            destination_name = spec.get('destination_name', spec.get('destination_table', 'output_table'))
            schema_name = None
            table_name = destination_name
            
            # Extract schema if format is "schema.table"
            if '.' in destination_name:
                schema_name, table_name = destination_name.split('.', 1)
            
            return f"""
        ### Output Data Saving (PostgreSQL Database)
        ```python
        # Use same database configuration as input
        DATABASE_URL = os.getenv('DATABASE_URL')
        ```
        - **Destination Type:** PostgreSQL Database  
        - **Connection:** Use same DATABASE_URL and connection parameters as input
        - **Target Table:** `{destination_name}`
        - **Schema Management:** 
          {f"- CREATE SCHEMA IF NOT EXISTS `{schema_name}`" if schema_name else "- Use default schema"}
          - Create table if not exists with proper column types
          - Handle schema and table creation before data insertion
        - **Write Mode:** Handle table creation, updates, or appends as specified
        - **Data Types:** Ensure proper mapping from DataFrame to SQL types
        - **Batch Processing:** Use efficient bulk insert methods (`to_sql` with `method='multi'`)
        - **Transaction Handling:** Use database transactions for data integrity
        - **Error Handling:** Handle schema/table creation errors gracefully
        - **Important:** Always create schema first, then table, then insert data
            """
        
        else:
            return f"""
        ### Output Data Saving (Generic)
        - **Destination Type:** {dest_type or 'Not specified'}
        - **Default:** Save to file-based output in pipeline directory
        - **Path:** `../pipelines/{spec.get('pipeline_name')}/output/`
            """
    