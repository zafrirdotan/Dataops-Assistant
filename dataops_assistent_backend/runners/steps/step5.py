# step5.py
import json
from pipeline_builder.deployment.pipeline_output_service import PipelineOutputService
from pipeline_builder.types import CodeGenResult
from shared.utils.spinner_utils import run_step_with_spinner


mock_pipeline_code: CodeGenResult = {
  "pipeline": "import os\nimport pandas as pd\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nimport logging\nfrom dotenv import load_dotenv\nimport glob\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom datetime import datetime\n\n# Global pipeline specification\nspec = {\n    \"pipeline_name\": \"ingest_csv_to_parquet_and_sqlite_20251225_1255\",\n    \"description\": \"Ingest CSV files from the data directory into a Parquet file partitioned by date and a SQLite table named orders_daily, running daily at 02:00.\",\n    \"source_type\": \"localFileCSV\",\n    \"source_table\": \"\",\n    \"source_path\": \"./data/*.csv\",\n    \"destination_type\": \"sqlite\",\n    \"destination_name\": \"orders_daily\",\n    \"transformation_logic\": \"\",\n    \"schedule\": \"0 2 * * *\"\n}\n\n# Configure logging\nPIPELINE_NAME = spec.get(\"pipeline_name\", \"unknown_pipeline\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format=f\"%(asctime)s %(levelname)s %(name)s [pipeline: {PIPELINE_NAME}] %(message)s\",\n    handlers=[\n        logging.FileHandler(\"pipeline.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n# Assumptions:\n# - All CSVs in the data folder have the same schema.\n# - The 'signup_date' column is used for partitioning Parquet files by date (YYYY-MM-DD).\n# - Sensitive paths (output, data) are set via environment variables.\n# - No transformation logic is specified, so data is loaded as-is.\n# - Data privacy: no sensitive data is logged.\n\n# extract_data function to extract data from source\n\ndef extract_data():\n    try:\n        data_folder = os.getenv('DATA_FOLDER', './data')\n        file_pattern = os.path.join(data_folder, '*.csv')\n        all_files = glob.glob(file_pattern)\n        if not all_files:\n            raise FileNotFoundError(f\"No CSV files found in {data_folder}\")\n        df_list = []\n        for file in all_files:\n            try:\n                df = pd.read_csv(file)\n                df_list.append(df)\n            except Exception as e:\n                logging.error(f\"Failed to read {file}: {str(e)}\")\n        if not df_list:\n            raise ValueError(\"No valid CSV files could be read.\")\n        data = pd.concat(df_list, ignore_index=True)\n        logging.info(f\"Extracted {len(data)} rows from {len(df_list)} CSV files.\")\n        return data\n    except Exception as e:\n        logging.error(f\"Error extracting data from CSV files: {str(e)}\")\n        return None\n\n# transform_data function to apply transformation logic\n\ndef transform_data(data):\n    # No transformation logic specified\n    return data\n\n# load_data function to load data to Parquet and SQLite\n\ndef load_data(data):\n    try:\n        output_folder = os.getenv('OUTPUT_FOLDER', './output_test')\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n        # Save as Parquet partitioned by signup_date (if exists)\n        if 'signup_date' in data.columns:\n            data['signup_date'] = pd.to_datetime(data['signup_date'], errors='coerce')\n            data['signup_date_str'] = data['signup_date'].dt.strftime('%Y-%m-%d')\n            for date_str, group in data.groupby('signup_date_str'):\n                if pd.isna(date_str):\n                    continue\n                partition_dir = os.path.join(output_folder, f\"parquet/signup_date={date_str}\")\n                os.makedirs(partition_dir, exist_ok=True)\n                table = pa.Table.from_pandas(group.drop(columns=['signup_date_str']))\n                pq.write_table(table, os.path.join(partition_dir, \"data.parquet\"))\n                logging.info(f\"Wrote {len(group)} rows to Parquet partition {partition_dir}\")\n        else:\n            # No partitioning column, write all to one file\n            table = pa.Table.from_pandas(data)\n            pq.write_table(table, os.path.join(output_folder, \"data.parquet\"))\n            logging.info(f\"Wrote {len(data)} rows to Parquet (no partitioning)\")\n        # Save to SQLite\n        table_name = spec['destination_name']\n        db_path = os.path.join(output_folder, f'{table_name}.sqlite')\n        engine = create_engine(f'sqlite:///{db_path}')\n        data.to_sql(table_name, con=engine, if_exists='replace', index=False)\n        logging.info(f\"Loaded {len(data)} rows into SQLite table {table_name} at {db_path}\")\n    except Exception as e:\n        logging.error(f\"Error loading data to destination: {str(e)}\")\n        raise\n\n# Main function to orchestrate the pipeline\ndef main():\n    load_dotenv()\n    try:\n        data = extract_data()\n        if data is not None:\n            transformed_data = transform_data(data)\n            if transformed_data is not None:\n                load_data(transformed_data)\n        return True\n    except Exception as e:\n        logging.exception(\"Pipeline execution failed\")\n        raise  # Show full error in console\n\nif __name__ == \"__main__\":\n    main()\n",
  "requirements": "pandas>=2.0.0\nnumpy>=1.24.0\nsqlalchemy>=2.0.0\npsycopg2-binary>=2.9.0\npyarrow>=14.0.0\nrequests>=2.31.0\npytest>=7.0.0\npython-dotenv>=1.0.0\nminio\n",
  "tests": "import pytest\nimport pandas as pd\nimport sys\nimport os\nimport tempfile\nimport shutil\nimport sqlite3\nimport glob\nimport json\n\n# Add the pipeline directory to the path\nsys.path.append(os.path.dirname(__file__))\n\ntry:\n    from pipeline import main\nexcept ImportError:\n    # Fallback if import fails\n    def main():\n        print(\"Pipeline main function not found\")\n        return True\n\ndef create_mock_csv_files(tmp_path, n=3):\n    # Create n mock CSV files in tmp_path\n    data = [\n        {'customer_id': 1.0, 'first_name': 'Robert', 'last_name': 'Butler', 'email': 'qlee@may.org', 'country': 'Kazakhstan', 'signup_date': '2025-01-31', 'is_active': True},\n        {'customer_id': 2.0, 'first_name': 'Mark', 'last_name': 'Villa', 'email': 'vasquezlori@henry-miranda.com', 'country': 'Reunion', 'signup_date': '2025-01-05', 'is_active': False},\n        {'customer_id': 3.0, 'first_name': 'Rachel', 'last_name': 'Mckinney', 'email': 'hblake@yahoo.com', 'country': 'Belize', 'signup_date': '2024-02-15', 'is_active': False},\n        {'customer_id': 4.0, 'first_name': 'Charles', 'last_name': 'Jones', 'email': 'nicole74@hotmail.com', 'country': 'Canada', 'signup_date': '2025-09-25', 'is_active': True},\n        {'customer_id': 5.0, 'first_name': 'Paul', 'last_name': 'James', 'email': 'anthonymartin@gmail.com', 'country': 'El Salvador', 'signup_date': '2024-04-24', 'is_active': True}\n    ]\n    df = pd.DataFrame(data)\n    for i in range(n):\n        file_path = tmp_path / f\"mock_data_{i}.csv\"\n        df.to_csv(file_path, index=False)\n    return tmp_path\n\ndef test_pipeline_execution(tmp_path, monkeypatch):\n    \"\"\"Test that the pipeline runs without errors and produces expected outputs.\"\"\"\n    # Setup mock data and environment\n    data_dir = tmp_path / \"data\"\n    output_dir = tmp_path / \"output\"\n    data_dir.mkdir()\n    output_dir.mkdir()\n    create_mock_csv_files(data_dir, n=2)\n    monkeypatch.setenv('DATA_FOLDER', str(data_dir))\n    monkeypatch.setenv('OUTPUT_FOLDER', str(output_dir))\n    # Run pipeline\n    result = main()\n    assert result is True or result is None\n    # Check SQLite output\n    db_path = output_dir / 'orders_daily.sqlite'\n    assert db_path.exists()\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(\"SELECT * FROM orders_daily\", conn)\n    assert not df.empty\n    assert 'customer_id' in df.columns\n    conn.close()\n    # Check Parquet output\n    parquet_dirs = glob.glob(str(output_dir / 'parquet' / 'signup_date=*'))\n    assert parquet_dirs, \"No Parquet partitions found\"\n    for part_dir in parquet_dirs:\n        files = glob.glob(os.path.join(part_dir, '*.parquet'))\n        assert files, f\"No Parquet file in {part_dir}\"\n    print(\"Pipeline executed and outputs validated successfully.\")\n\ndef test_data_validation(tmp_path, monkeypatch):\n    \"\"\"Test basic data validation.\"\"\"\n    data_dir = tmp_path / \"data\"\n    output_dir = tmp_path / \"output\"\n    data_dir.mkdir()\n    output_dir.mkdir()\n    create_mock_csv_files(data_dir, n=1)\n    monkeypatch.setenv('DATA_FOLDER', str(data_dir))\n    monkeypatch.setenv('OUTPUT_FOLDER', str(output_dir))\n    main()\n    db_path = output_dir / 'orders_daily.sqlite'\n    conn = sqlite3.connect(db_path)\n    df = pd.read_sql_query(\"SELECT * FROM orders_daily\", conn)\n    assert df['customer_id'].notnull().all()\n    assert df['email'].str.contains('@').all()\n    conn.close()\n    print(\"Data validation passed.\")\n\nif __name__ == \"__main__\":\n    import tempfile\n    import pytest\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        class DummyMonkeyPatch:\n            def setenv(self, key, value):\n                os.environ[key] = value\n        tmp_path = type('tmp_path', (), {'__truediv__': lambda self, x: os.path.join(tmpdirname, x), 'mkdir': lambda self: None, '__str__': lambda self: tmpdirname})()\n        monkeypatch = DummyMonkeyPatch()\n        test_pipeline_execution(tmp_path, monkeypatch)\n        test_data_validation(tmp_path, monkeypatch)\n        print(\"All tests passed!\")\n"
}

spec = {
    "pipeline_name": "mock_etl_pipeline"
}


async def main(*args):
    step_msg = "Step 5 executed. Store generated pipeline files."
    step_number = 5
    output_service = PipelineOutputService()
    pipeline_info, error = await run_step_with_spinner(step_msg, step_number, output_service.store_pipeline_files,
                spec.get("pipeline_name"), 
                mock_pipeline_code, 
            )
    
    print(step_msg)
    print("Pipeline Info:", json.dumps(pipeline_info))
    print("Error:", error)
